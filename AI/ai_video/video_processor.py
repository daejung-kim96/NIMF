"""
ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬ ëª¨ë“ˆ
@module video_processor
@author joon hyeok
@date 2025-01-08
@description ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ëŠ” VideoProcessor í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
"""

import asyncio
import numpy as np
import cv2
import json
import threading
import queue
from time import time
from datetime import datetime
from typing import List, Dict, Optional, Callable
from aiortc import MediaStreamTrack
from av import VideoFrame

from .object_detector import (
    YOLODetector,
    DetectionFilter,
    DetectionVisualizer,
    DetectionResult
)
from config import config
from session_state_manager import session_state_manager

# í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
CLASS_CATEGORY_MAPPING = {
    0: 'ìŒì£¼',
    1: 'ìŒì£¼',
    2: 'ë‚ ì¹´ë¡œìš´ ë„êµ¬',
    3: 'í¡ì—°',
    4: 'ë‚ ì¹´ë¡œìš´ ë„êµ¬',
    5: 'ë‚ ì¹´ë¡œìš´ ë„êµ¬',
    6: 'í™”ê¸°ë¥˜',
    7: 'ì´ê¸°ë¥˜',
    8: 'í™”ê¸°ë¥˜'
}

# í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘ (class_id -> class_name)
CLASS_NAMES = {
    0: 'ìˆ ',
    1: 'ìˆ ì”', 
    2: 'ë“œë¼ì´ë²„',
    3: 'ë‹´ë°°',
    4: 'ì»¤í„°ì¹¼',
    5: 'ì¹¼',
    6: 'ë¶ˆ',
    7: 'ì´',
    8: 'ë¼ì´í„°'
}

# ì „ì—­ YOLO ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
_global_yolo_detector = None
_global_model_path = None

def initialize_global_yolo_model():
    """
    ì „ì—­ YOLO ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    @param {str} model_path - YOLO ëª¨ë¸ ê²½ë¡œ
    @returns {bool} ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    """
    global _global_yolo_detector, _global_model_path
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´
    model_path = config.get_yolo_model_path()
    
    try:
        print(f"ğŸ”§ ì „ì—­ YOLO ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘: {model_path}")
        _global_yolo_detector = YOLODetector(model_path=model_path, confidence_threshold=config.OBJECT_DETECTION_CONFIDENCE)
        
        if _global_yolo_detector.initialize():
            _global_model_path = model_path
            print(f"âœ… ì „ì—­ YOLO ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_path}")
            return True
        else:
            print("âŒ ì „ì—­ YOLO ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            _global_yolo_detector = None
            return False
            
    except Exception as e:
        print(f"âŒ ì „ì—­ YOLO ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        _global_yolo_detector = None
        return False

def get_global_yolo_detector():
    """
    ì „ì—­ YOLO ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    @returns {YOLODetector|None} ì „ì—­ YOLO ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    return _global_yolo_detector

def is_global_yolo_initialized():
    """
    ì „ì—­ YOLO ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    @returns {bool} ì´ˆê¸°í™” ì—¬ë¶€
    """
    return _global_yolo_detector is not None


def _categories_to_enabled_class_ids(video_category_flags: Dict[str, bool]) -> list:
    """
    ì„¸ì…˜ì˜ videoFilter.category í”Œë˜ê·¸ë¥¼ YOLO í´ë˜ìŠ¤ ID ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì¹´í…Œê³ ë¦¬ í‚¤ ì˜ˆ: smoke, drink, sharpObjects, flammables, firearms, exposure
    """
    if not video_category_flags:
        return []

    # ì¹´í…Œê³ ë¦¬ â†’ í´ë˜ìŠ¤ ID ë§¤í•‘ ì •ì˜
    category_to_class_ids = {
        # í¡ì—°
        'smoke': [3],
        # ìŒì£¼
        'drink': [0, 1],
        # ë‚ ì¹´ë¡œìš´ ë„êµ¬
        'sharpObjects': [2, 4, 5],
        # ì¸í™”ë¬¼/ê°€ì—°ë¬¼(ë¶ˆ, ë¼ì´í„° í¬í•¨)
        'flammables': [6, 8],
        # ì´ê¸°ë¥˜
        'firearms': [7],
        # ë…¸ì¶œ(í˜„ì¬ í•´ë‹¹ í´ë˜ìŠ¤ ì—†ìŒ)
        'exposure': [],
    }

    enabled_ids = set()
    for category_key, is_enabled in video_category_flags.items():
        if is_enabled:
            enabled_ids.update(category_to_class_ids.get(category_key, []))
    return sorted(enabled_ids)


class VideoProcessor:
    """
    ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬ í´ë˜ìŠ¤
    
    ë¹„ë””ì˜¤ í”„ë ˆì„ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    data_channel = None

    def __init__(self):
        """
        VideoProcessor ì´ˆê¸°í™”
        
        """
        self.frame_count = 0
        self.processing_stats = {
            'total_frames': 0,
            'processed_frames': 0,
            'processing_time': 0.0,
            'detection_time': 0.0,
            'objects_detected': 0,
            'avg_fps': 0.0
        }
        
        # FPS ê³„ì‚°ì„ ìœ„í•œ ê°„ë‹¨í•œ ìƒíƒœ
        self._fps_start_time = None
        self._fps_processed_frames = 0
        self._fps_logged_30s = False
        
        # ByteTrack ID ê´€ë¦¬
        self.max_track_id = -1  # ì§€ê¸ˆê¹Œì§€ ë³¸ ê°€ì¥ í° track_id
        
        # ì„¸ì…˜ ID (ì„¸ì…˜ë³„ í•„í„° ì ìš©ìš©)
        self.session_id: Optional[str] = None

        # ë¬¼ì²´ ê°ì§€ ê´€ë ¨ ì»´í¬ë„ŒíŠ¸
        self.enable_object_detection = True
        self.object_detector = None
        self.detection_filter = DetectionFilter()
        self.visualizer = DetectionVisualizer()
        self.current_detections: List[DetectionResult] = []
        # í´ë˜ìŠ¤ë³„ë¡œ ì´ë¯¸ ì „ì†¡í•œ ByteTrack ID ì§‘í•© ì €ì¥: { class_id: set(track_ids) }
        self.seen_track_ids_by_class: Dict[int, set] = {}
        
        # ë¬¼ì²´ ê°ì§€ ì½œë°± í•¨ìˆ˜ë“¤
        self.detection_callbacks: List[Callable[[List[DetectionResult]], None]] = []
        
        # ë³„ë„ ìŠ¤ë ˆë“œ ì²˜ë¦¬ë¥¼ ìœ„í•œ íì™€ ìŠ¤ë ˆë“œ
        self.processed_frame_queue = queue.Queue(maxsize=10)  # ì²˜ë¦¬ëœ í”„ë ˆì„ í
        self.last_processed_frame = None  # ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ í”„ë ˆì„ (ë³µì œìš©)
        self.processing_thread = None
        self.processing_thread_running = False
        
        # ì¶œë ¥ ìŠ¤ë¬´ë”©ì„ ìœ„í•œ ì¶œë ¥ ë²„í¼ í (ì´ë¯¸ì§€+íƒ€ì´ë° í˜ì–´ë¡œ ì €ì¥)
        self.output_frame_queue = queue.Queue(maxsize=120)  # (img, pts, time_base)
        self.output_buffer_target = 15  # ì •ì  êµ¬ê°„ì—ì„œ ë²„í¼ ëª©í‘œ í¬ê¸°
        
        # ê°ì§€ ì£¼ê¸°: Ní”„ë ˆì„ë§ˆë‹¤ 1íšŒ ê°ì§€, ë‚˜ë¨¸ì§€ëŠ” ì§ì „ ê²°ê³¼ ì¬ì‚¬ìš©
        self.detection_stride = 3
        self._worker_frame_index = 0
        
        # ëª¨ì…˜ ê¸°ë°˜ ìŠ¤í‚µ ì„¤ì •
        self.motion_enabled = True
        self.motion_threshold = 0.02  # ë³€ê²½ í”½ì…€ ë¹„ìœ¨ ì„ê³„ì¹˜ (2%)
        self.motion_downscale = (160, 90)  # ëª¨ì…˜ ê³„ì‚°ìš© ë‹¤ìš´ìŠ¤ì¼€ì¼ í•´ìƒë„
        self.max_skip_without_detection = self.detection_stride * 3  # ì•ˆì „ ì£¼ê¸°
        self._prev_motion_frame_small = None
        self._frames_since_last_detection = self.detection_stride  # ì´ˆê¸° ê°ì§€ í—ˆìš©ì„ ìœ„í•´ strideë§Œí¼ ì±„ì›€

        # ë™ì  ìŠ¤íŠ¸ë¼ì´ë“œ ì„¤ì •
        self.dynamic_stride_enabled = True
        self.min_detection_stride = 1
        self.max_detection_stride = 10
        # íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì„ê³„ê°’(EMA ê¸°ì¤€): ë†’ì„ìˆ˜ë¡ ë” ìì£¼ ì¤„ì´ê³ , ë‚®ì„ìˆ˜ë¡ ë” ìì£¼ ëŠ˜ë¦¼
        self.high_motion_threshold = 0.05
        self.low_motion_threshold = 0.01
        # ëª¨ì…˜ EMA ì„¤ì •
        self._ema_motion = 0.0
        self._ema_alpha = 0.3
        # ìŠ¤íŠ¸ë¼ì´ë“œ ë³€ê²½ ì¿¨ë‹¤ìš´
        self._stride_cooldown_frames = 5
        self._frames_since_last_stride_update = self._stride_cooldown_frames

        # ëª¨ì…˜ ì˜¨ì…‹ ë²„ìŠ¤íŠ¸ ë° ëª¨ì…˜ ì¤‘ ìµœì†Œ ê°„ê²© ì„¤ì •
        self.motion_stride = 1  # ëª¨ì…˜ ì¤‘ ìµœì†Œ ê°ì§€ ê°„ê²©(í”„ë ˆì„)
        self.motion_burst_enabled = True
        self.motion_burst_frames = 3  # ëª¨ì…˜ ì‹œì‘ ì‹œ ì—°ì† ê°ì§€ í”„ë ˆì„ ìˆ˜
        self._motion_burst_remaining = 0
        self._motion_prev_above = False
        
        # ë™ì  ë¸”ëŸ¬ ìƒ˜í”Œë§ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
        self._frames_since_last_blur_draw = 0
        self._last_blurred_image = None

        # ë¬¼ì²´ ê°ì§€ ì´ˆê¸°í™”
        self._initialize_object_detection()
        
        # ê¸°ë³¸: ë¸”ëŸ¬ ë¹„í™œì„±í™” (videofilter.action.filtering=trueì¼ ë•Œë§Œ í™œì„±í™”)
        self.visualizer.enable_object_blur(enable=False, blur_classes=None, blur_strength=self.visualizer.blur_strength)
        print("ğŸ”“ ê¸°ë³¸ ë¸”ëŸ¬ ë¹„í™œì„±í™” (videofilter.action.filtering=trueì¼ ë•Œë§Œ ì ìš©)")
        
        # ê¸°ë³¸: í´ë˜ìŠ¤ í•„í„° ì‚¬ìš© + í—ˆìš© ì§‘í•© ë¹„ì›Œì„œ(=ëª¨ë‘ ì°¨ë‹¨) ì‹œì‘
        self.detection_filter.use_class_filter = True
        self.detection_filter.set_class_filter([])

        # ë³„ë„ ìŠ¤ë ˆë“œ ì‹œì‘
        self._start_processing_thread()
    
    def _start_processing_thread(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.processing_thread_running:
            return
        
        self.processing_thread_running = True
        self.processing_thread = threading.Thread(target=self._processing_thread_worker, daemon=True)
        self.processing_thread.start()
        print("ğŸ”„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ ì²˜ë¦¬ ì‹œì‘")
    
    def _stop_processing_thread(self):
        """ë³„ë„ ìŠ¤ë ˆë“œë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.processing_thread_running = False
        if self.processing_thread and self.processing_thread.is_alive():
            self.processing_thread.join(timeout=1.0)
        print("ğŸ›‘ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ì§€")
    
    def _processing_thread_worker(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” í”„ë ˆì„ ì²˜ë¦¬ ì›Œì»¤"""
        while self.processing_thread_running:
            try:
                # íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (1ì´ˆ íƒ€ì„ì•„ì›ƒ)
                frame_data = self.processed_frame_queue.get(timeout=1.0)
                if frame_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                img, original_frame = frame_data
                
                # í”„ë ˆì„ ì¸ë±ìŠ¤ ì¦ê°€ (ì›Œì»¤ ê¸°ì¤€)
                self._worker_frame_index += 1
                
                # ë¬¼ì²´ ê°ì§€ ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ)
                if self.enable_object_detection:
                    detections = []
                    # ëª¨ì…˜ ë¹„ìœ¨ ê³„ì‚°
                    motion_ratio = 0.0
                    if self.motion_enabled:
                        try:
                            motion_ratio = self._compute_motion_ratio(img)
                        except Exception as _:
                            motion_ratio = 0.0
                    
                    # ê°ì§€ ì‹¤í–‰ ì¡°ê±´
                    allow_window = (self._frames_since_last_detection >= self.detection_stride)
                    safety_due = (self._frames_since_last_detection >= self.max_skip_without_detection)
                    motion_trigger = (motion_ratio >= self.motion_threshold)

                    # ëª¨ì…˜ ì˜¨ì…‹(burst) ê°ì§€: ì„ê³„ì¹˜ í•˜->ìƒ êµì°¨ ì‹œ ì¦‰ì‹œ ëª‡ í”„ë ˆì„ ì—°ì† ê°ì§€
                    if self.motion_burst_enabled:
                        if (not self._motion_prev_above) and motion_trigger:
                            self._motion_burst_remaining = self.motion_burst_frames
                        self._motion_prev_above = motion_trigger

                    in_burst = (self._motion_burst_remaining > 0)

                    # ëª¨ì…˜ ì¤‘ ìµœì†Œ ê°„ê²© ì ìš©: ëª¨ì…˜ì´ ê³„ì†ë˜ëŠ” ë™ì•ˆì—” motion_stride ê¸°ì¤€ í—ˆìš©
                    motion_window = (self._frames_since_last_detection >= self.motion_stride) if motion_trigger else allow_window

                    run_detection = (motion_trigger and motion_window) or in_burst or safety_due
                    
                    if run_detection:
                        detections = self._detect_objects_thread_safe(img)
                        self._frames_since_last_detection = 0
                        if in_burst:
                            self._motion_burst_remaining = max(0, self._motion_burst_remaining - 1)
                    else:
                        detections = self.current_detections
                        self._frames_since_last_detection += 1
                    
                    # ê°ì§€ ê²°ê³¼ ì‹œê°í™” (ë™ì  ë¸”ëŸ¬ ìƒ˜í”Œë§)
                    if detections:
                        do_draw = True
                        if not motion_trigger:
                            # ì •ì  êµ¬ê°„: ë§¤ Ní”„ë ˆì„ë§ˆë‹¤ ìƒˆë¡œ ë¸”ëŸ¬ ê³„ì‚°í•˜ê³ , ê·¸ ì™¸ì—ëŠ” ìºì‹œëœ ë¸”ëŸ¬ ì´ë¯¸ì§€ ì¬ì‚¬ìš©
                            self._frames_since_last_blur_draw = getattr(self, "_frames_since_last_blur_draw", 0) + 1
                            static_n = getattr(self, "blur_sample_static_n", 5)
                            do_draw = (self._frames_since_last_blur_draw >= static_n)

                        if do_draw or motion_trigger:
                            # ë™ì  êµ¬ê°„ì€ í•­ìƒ ìƒˆë¡œ ë¸”ëŸ¬, ì •ì  êµ¬ê°„ì€ ìƒ˜í”Œë§ ê°„ê²©ë§ˆë‹¤ ë¸”ëŸ¬ ê°±ì‹ 
                            print(f"âœ… {len(detections)}ê°œ ë¬¼ì²´ ê°ì§€ë¨ (ì¬ì‚¬ìš© í¬í•¨)")
                            blurred = self.visualizer.draw_detections(img, detections)
                            self._last_blurred_image = blurred
                            img = blurred
                            self._frames_since_last_blur_draw = 0 if not motion_trigger else self._frames_since_last_blur_draw
                        else:
                            # ì •ì  êµ¬ê°„ ìƒ˜í”Œë§ í”„ë ˆì„ì´ ì•„ë‹ˆë©´ ì´ì „ ë¸”ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ í•­ìƒ ë¸”ëŸ¬ ìƒíƒœ ìœ ì§€
                            if self._last_blurred_image is not None:
                                img = self._last_blurred_image
                            else:
                                # ìºì‹œê°€ ì—†ìœ¼ë©´ í•œ ë²ˆ ìƒì„±
                                blurred = self.visualizer.draw_detections(img, detections)
                                self._last_blurred_image = blurred
                                img = blurred
                        # img = self.visualizer.draw_detection_count(img, detections)
                    # else:
                    #     print("ğŸ“­ ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì—†ìŒ")

                    # ë™ì  ìŠ¤íŠ¸ë¼ì´ë“œ ì—…ë°ì´íŠ¸
                    if self.dynamic_stride_enabled:
                        self._update_dynamic_stride(motion_ratio)
                
                # í‘œì¤€ í•´ìƒë„(1280x720)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                try:
                    img = cv2.resize(img, (1280, 720))
                except Exception as e:
                    print(f"ë¦¬ì‚¬ì´ì¦ˆ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ VideoFrameìœ¼ë¡œ ë³€í™˜
                # ì¶œë ¥ ìŠ¤ë¬´ë”©: ì²˜ë¦¬ ì´ë¯¸ì§€ì™€ ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í•¨ê»˜ íì— ì €ì¥
                try:
                    self.output_frame_queue.put_nowait((img, original_frame.pts, original_frame.time_base))
                except queue.Full:
                    try:
                        _ = self.output_frame_queue.get_nowait()
                        self.output_frame_queue.put_nowait((img, original_frame.pts, original_frame.time_base))
                    except Exception:
                        pass
                
                # ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ í”„ë ˆì„ ì—…ë°ì´íŠ¸: ì¦‰ì‹œ ì „ì†¡ ê²½ë¡œë¥¼ ìœ„í•´ì„œë„ ìœ ì§€ (fallback)
                try:
                    processed_frame = VideoFrame.from_ndarray(img, format='bgr24')
                    processed_frame.pts = original_frame.pts
                    processed_frame.time_base = original_frame.time_base
                    self.last_processed_frame = processed_frame
                except Exception:
                    pass
                
                # í‰ê·  FPS ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ëœ í”„ë ˆì„ ìˆ˜ / ê²½ê³¼ ì‹œê°„)
                now = time()
                if self._fps_start_time is None:
                    self._fps_start_time = now
                self._fps_processed_frames += 1
                elapsed = max(now - self._fps_start_time, 1e-6)
                self.processing_stats['avg_fps'] = self._fps_processed_frames / elapsed
                
                # ìµœì´ˆ 30ì´ˆ ê²½ê³¼ í›„ í•œ ë²ˆë§Œ í‰ê·  FPS ì¶œë ¥
                if not self._fps_logged_30s and elapsed >= 30.0:
                    print(f"ğŸ“ˆ í‰ê·  ì²˜ë¦¬ FPS(30ì´ˆ): {self.processing_stats['avg_fps']:.2f}")
                    self._fps_logged_30s = True
                
                # print(f"ğŸ”„ í”„ë ˆì„ ì²˜ë¦¬ ì™„ë£Œ (ìŠ¤ë ˆë“œ): {self.frame_count}")
                
            except queue.Empty:
                continue  # íƒ€ì„ì•„ì›ƒ ì‹œ ê³„ì† ëŒ€ê¸°
            except Exception as e:
                print(f"âŒ ë³„ë„ ìŠ¤ë ˆë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    def _detect_objects_thread_safe(self, img: np.ndarray) -> List[DetectionResult]:
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ë¬¼ì²´ ê°ì§€ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œ)"""
        if not self.enable_object_detection:
            return []
        
        if not self.object_detector:
            return []
        
        start_time = time()
        
        try:
            # ë¬¼ì²´ ê°ì§€ ì‹¤í–‰
            detections = self.object_detector.detect(img)
            
            # í•„í„°ë§ ì ìš©
            filtered_detections = self.detection_filter.filter_detections(detections)
            
            # Data Channel ì „ì†¡ì€ JSON í˜•íƒœë¡œë§Œ ê°€ëŠ¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì „ì†¡í•˜ì§€ ì•ŠìŒ
            # ëŒ€ì‹  _send_detection_results_via_data_channelì—ì„œ ì²˜ë¦¬

            # í†µê³„ ì—…ë°ì´íŠ¸
            detection_time = time() - start_time
            self.processing_stats['detection_time'] += detection_time
            self.processing_stats['objects_detected'] += len(filtered_detections)
            
            # í˜„ì¬ ê°ì§€ ê²°ê³¼ ì €ì¥
            self.current_detections = filtered_detections
            
            # Data Channel ì „ì†¡ì€ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê±´ë„ˆëœ€
            # ëŒ€ì‹  ê°ì§€ ê²°ê³¼ë§Œ ì €ì¥í•˜ê³  ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬
            
            return filtered_detections
            
        except Exception as e:
            print(f"ë¬¼ì²´ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def _compute_motion_ratio(self, img: np.ndarray) -> float:
        """ì €í•´ìƒë„ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì°¨ë¶„ìœ¼ë¡œ í”„ë ˆì„ ê°„ ëª¨ì…˜ ë¹„ìœ¨(0~1)ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ë‹¤ìš´ìŠ¤ì¼€ì¼ ë° ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        try:
            small = cv2.resize(img, self.motion_downscale)
            gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
        except Exception:
            return 0.0

        # ì²« í”„ë ˆì„ ì²˜ë¦¬
        if self._prev_motion_frame_small is None:
            self._prev_motion_frame_small = gray
            return 1.0  # ì²« í”„ë ˆì„ì€ ê°•ì œ ê°ì§€ ìœ ë„

        # ì ˆëŒ€ ì°¨ì´ ë° ì´ì§„í™”
        diff = cv2.absdiff(self._prev_motion_frame_small, gray)
        # ë…¸ì´ì¦ˆ ì–µì œë¥¼ ìœ„í•œ ì„ê³„ê°’(ì¡°ì • ê°€ëŠ¥)
        _, thresh = cv2.threshold(diff, 20, 255, cv2.THRESH_BINARY)

        # ë³€ê²½ í”½ì…€ ë¹„ìœ¨ ê³„ì‚°
        changed = int(np.count_nonzero(thresh))
        total = thresh.size
        ratio = changed / max(1, total)

        # ì´ì „ í”„ë ˆì„ ì—…ë°ì´íŠ¸
        self._prev_motion_frame_small = gray
        return float(ratio)

    def _update_dynamic_stride(self, motion_ratio: float) -> None:
        """ëª¨ì…˜ EMAë¥¼ ê¸°ë°˜ìœ¼ë¡œ detection_strideë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•©ë‹ˆë‹¤."""
        # EMA ì—…ë°ì´íŠ¸
        self._ema_motion = (self._ema_alpha * motion_ratio) + ((1.0 - self._ema_alpha) * self._ema_motion)

        # ì¿¨ë‹¤ìš´ ì²´í¬
        if self._frames_since_last_stride_update < self._stride_cooldown_frames:
            self._frames_since_last_stride_update += 1
            return

        # íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ê¸°ë°˜ ì¡°ì •
        new_stride = self.detection_stride
        if self._ema_motion >= self.high_motion_threshold:
            new_stride = max(self.min_detection_stride, self.detection_stride - 1)
        elif self._ema_motion <= self.low_motion_threshold:
            new_stride = min(self.max_detection_stride, self.detection_stride + 1)

        if new_stride != self.detection_stride:
            self.detection_stride = new_stride
            # ì•ˆì „ ì£¼ê¸°ë„ í•¨ê»˜ ì¡°ì • (ë¹„ë¡€ì ìœ¼ë¡œ)
            self.max_skip_without_detection = max(self.detection_stride * 2, min(self.detection_stride * 5, self.max_detection_stride * 3))
            self._frames_since_last_stride_update = 0
    
    def _initialize_object_detection(self):
        """
        ë¬¼ì²´ ê°ì§€ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ì „ì—­ YOLO ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        if not self.enable_object_detection:
            print("âš ï¸ ë¬¼ì²´ ê°ì§€ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        # ì „ì—­ YOLO ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        self.object_detector = get_global_yolo_detector()
        
        if self.object_detector:
            print("âœ… ë¬¼ì²´ ê°ì§€ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   ëª¨ë¸ ê²½ë¡œ: {_global_model_path}")
        else:
            print("âŒ ë¬¼ì²´ ê°ì§€ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            print("   ì „ì—­ YOLO ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì„¸ì…˜ë³„ ê¸°ì¡´ ì„¤ì •ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜ì˜
        if self.session_id:
            try:
                category_flags = session_state_manager.get_video_filter(self.session_id)
                enabled_ids = _categories_to_enabled_class_ids(category_flags)
                self.detection_filter.use_class_filter = True
                self.detection_filter.set_class_filter(enabled_ids)
                print(f"ğŸ¯ ì„¸ì…˜ {self.session_id} í´ë˜ìŠ¤ í•„í„° ì´ˆê¸° ì ìš©: {enabled_ids}")
                # ë¸”ëŸ¬ í”Œë˜ê·¸ë„ ì ìš©
                self._apply_blur_flag_from_session(self.session_id)
            except Exception:
                pass
    
    def process_frame(self, frame: VideoFrame) -> VideoFrame:
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ë³„ë„ ìŠ¤ë ˆë“œì— í”„ë ˆì„ì„ ì „ë‹¬í•˜ê³  ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @param {VideoFrame} frame - ì²˜ë¦¬í•  ë¹„ë””ì˜¤ í”„ë ˆì„
        @returns {VideoFrame} ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ í”„ë ˆì„ (ì¦‰ì‹œ ë°˜í™˜)
        """
        start_time = time()
        
        try:
            # í”„ë ˆì„ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            img = frame.to_ndarray(format='bgr24')
            
            # í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€
            self.frame_count += 1
            
            # ë³„ë„ ìŠ¤ë ˆë“œì— í”„ë ˆì„ ì „ë‹¬ (íê°€ ê°€ë“ ì°¬ ê²½ìš° ê¸°ì¡´ í•­ëª© ì œê±°)
            try:
                if self.processed_frame_queue.full():
                    try:
                        self.processed_frame_queue.get_nowait()  # ê¸°ì¡´ í•­ëª© ì œê±°
                    except queue.Empty:
                        pass
                
                self.processed_frame_queue.put_nowait((img, frame))
                # print(f"ğŸ“¤ í”„ë ˆì„ì„ ë³„ë„ ìŠ¤ë ˆë“œì— ì „ë‹¬: {self.frame_count}")
            except queue.Full:
                print(f"âš ï¸ í”„ë ˆì„ íê°€ ê°€ë“ ì°¸, í”„ë ˆì„ ìŠ¤í‚µ: {self.frame_count}")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(time() - start_time)
            
            # ì¦‰ì‹œ ì›ë³¸ í”„ë ˆì„ ë°˜í™˜ (ì²˜ë¦¬ëŠ” ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ)
            return frame
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return frame
    
    def get_processed_frame(self) -> Optional[VideoFrame]:
        """
        ì²˜ë¦¬ëœ í”„ë ˆì„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì¶œë ¥ ë²„í¼ íì— í”„ë ˆì„ì´ ìˆìœ¼ë©´ íƒ€ì´ë° íì˜ PTS/time_baseì™€ ë§¤ì¹­í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.
        ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ì´ì „ í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {VideoFrame|None} ì²˜ë¦¬ëœ í”„ë ˆì„ ë˜ëŠ” ì´ì „ í”„ë ˆì„ ë³µì œë³¸
        """
        # ì¶œë ¥ ë²„í¼ ìš°ì„  ì‚¬ìš© (ì´ë¯¸ì§€+íƒ€ì´ë° ê°™ì´ ë³´ê´€)
        try:
            if not self.output_frame_queue.empty():
                img, pts, time_base = self.output_frame_queue.get_nowait()
                vf = VideoFrame.from_ndarray(img, format='bgr24')
                vf.pts = pts
                vf.time_base = time_base
                # ìµœì‹  í”„ë ˆì„ìœ¼ë¡œë„ ë³´ê´€ (fallback ëŒ€ë¹„)
                self.last_processed_frame = vf
                return vf
        except Exception:
            pass
        
        if self.last_processed_frame is not None:
            return self.last_processed_frame
        return None
    

    
    def add_detection_callback(self, callback: Callable[[List[DetectionResult]], None]):
        """
        ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        @param {Callable} callback - ê°ì§€ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•  ì½œë°± í•¨ìˆ˜
        """
        self.detection_callbacks.append(callback)
    
    def remove_detection_callback(self, callback: Callable[[List[DetectionResult]], None]):
        """
        ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        @param {Callable} callback - ì œê±°í•  ì½œë°± í•¨ìˆ˜
        """
        if callback in self.detection_callbacks:
            self.detection_callbacks.remove(callback)
    
    def set_detection_filter(self, enabled_classes: Optional[List[int]] = None, 
                           confidence_range: Optional[tuple] = None,
                           area_range: Optional[tuple] = None):
        """
        ê°ì§€ í•„í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {List[int]} enabled_classes - í™œì„±í™”í•  í´ë˜ìŠ¤ ID ëª©ë¡
        @param {tuple} confidence_range - ì‹ ë¢°ë„ ë²”ìœ„ (min, max)
        @param {tuple} area_range - ë©´ì  ë²”ìœ„ (min, max)
        """
        if enabled_classes is not None:
            self.detection_filter.set_class_filter(enabled_classes)
        
        if confidence_range is not None:
            min_conf, max_conf = confidence_range
            self.detection_filter.set_confidence_range(min_conf, max_conf)
        
        if area_range is not None:
            min_area, max_area = area_range
            self.detection_filter.set_area_range(min_area, max_area)
    
    def get_current_detections(self) -> List[DetectionResult]:
        """
        í˜„ì¬ í”„ë ˆì„ì˜ ê°ì§€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {List[DetectionResult]} í˜„ì¬ ê°ì§€ ê²°ê³¼
        """
        return self.current_detections.copy()
    
    def get_detection_stats(self) -> Dict:
        """
        ë¬¼ì²´ ê°ì§€ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {Dict} ê°ì§€ í†µê³„
        """
        if not self.object_detector:
            return {}
        
        stats = self.object_detector.get_stats()
        stats.update({
            'detection_time': self.processing_stats['detection_time'],
            'objects_detected': self.processing_stats['objects_detected']
        })
        return stats
    
    def reset_detection_stats(self):
        """
        ë¬¼ì²´ ê°ì§€ í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        if self.object_detector:
            self.object_detector.reset_stats()
        
        self.processing_stats['detection_time'] = 0.0
        self.processing_stats['objects_detected'] = 0
    
    def enable_object_blur(self, enable: bool = True, blur_classes: Optional[List[int]] = None, blur_strength: int = 50):
        """
        ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {bool} enable - ë¸”ëŸ¬ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        @param {List[int]} blur_classes - ë¸”ëŸ¬ ì ìš©í•  í´ë˜ìŠ¤ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  í´ë˜ìŠ¤)
        @param {int} blur_strength - ë¸”ëŸ¬ ê°•ë„ (í™€ìˆ˜ ê¶Œì¥)
        """
        self.visualizer.enable_object_blur(enable, blur_classes, blur_strength)
        if enable:
            print(f"ğŸ”’ ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ê°•ë„: {blur_strength})")
            if blur_classes:
                print(f"   ë¸”ëŸ¬ ì ìš© í´ë˜ìŠ¤: {blur_classes}")
            else:
                print("   ëª¨ë“  ê°ì§€ëœ ë¬¼ì²´ì— ë¸”ëŸ¬ ì ìš©")
        else:
            print("ğŸ”“ ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _apply_basic_processing(self, img: np.ndarray) -> np.ndarray:
        """
        ê¸°ë³¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‘ì—…ì„ ì ìš©í•©ë‹ˆë‹¤.
        ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
        
        @param {np.ndarray} img - ì²˜ë¦¬í•  ì´ë¯¸ì§€
        @returns {np.ndarray} ì²˜ë¦¬ëœ ì´ë¯¸ì§€
        """
        processed_img = img.copy()
        
        # ë¬¼ì²´ ê°ì§€ ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
        if self.enable_object_detection:
            print(f"ğŸ” ë¬¼ì²´ ê°ì§€ ì‹¤í–‰ ì¤‘... (í”„ë ˆì„ {self.frame_count})")
            detections = self._detect_objects_thread_safe(processed_img)
            
            # ê°ì§€ ê²°ê³¼ ì‹œê°í™” (ì˜µì…˜)
            if detections:
                print(f"âœ… {len(detections)}ê°œ ë¬¼ì²´ ê°ì§€ë¨")
                processed_img = self.visualizer.draw_detections(processed_img, detections)
                processed_img = self.visualizer.draw_detection_count(processed_img, detections)
            else:
                print("ğŸ“­ ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì—†ìŒ")
        
        # í‘œì¤€ í•´ìƒë„(1280x720)ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ raw íŒŒì´í”„ ê·œê²©ì— ë§ì¶¤
        try:
            processed_img = cv2.resize(processed_img, (1280, 720))
        except Exception as e:
            print(f"ë¦¬ì‚¬ì´ì¦ˆ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì—¬ê¸°ì— ì¶”ê°€ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        # ì˜ˆ: ë¦¬ì‚¬ì´ì¦ˆ, í•„í„°ë§, íš¨ê³¼ ì ìš© ë“±
        
        return processed_img
    
    def _update_stats(self, processing_time: float):
        """
        ì²˜ë¦¬ í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        @param {float} processing_time - ì²˜ë¦¬ ì‹œê°„
        """
        self.processing_stats['total_frames'] += 1
        self.processing_stats['processed_frames'] += 1
        self.processing_stats['processing_time'] += processing_time
    
    def get_stats(self) -> dict:
        """
        ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {dict} ì²˜ë¦¬ í†µê³„
        """
        stats = self.processing_stats.copy()
        if stats['processed_frames'] > 0:
            stats['avg_processing_time'] = (
                stats['processing_time'] / stats['processed_frames']
            )
        return stats
    
    def reset_stats(self):
        """
        ì²˜ë¦¬ í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.processing_stats = {
            'total_frames': 0,
            'processed_frames': 0,
            'processing_time': 0.0,
            'detection_time': 0.0,
            'objects_detected': 0,
            'avg_fps': 0.0
        }
        
        # FPS ìƒíƒœ ì´ˆê¸°í™”
        self._fps_start_time = None
        self._fps_processed_frames = 0
        self._fps_logged_30s = False
    
    def set_data_channel(self, data_channel):
        """
        Data Channelì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {RTCDataChannel} data_channel - ì„¤ì •í•  Data Channel
        """
        self.data_channel = data_channel
        print(f"ğŸ“¡ VideoProcessorì— Data Channel ì„¤ì •ë¨: {data_channel.label}")
        
        # ë¬¼ì²´ ê°ì§€ê¸°ì—ë„ Data Channel ì „ë‹¬
        if hasattr(self, 'object_detector') and self.object_detector:
            if hasattr(self.object_detector, 'set_data_channel'):
                self.object_detector.set_data_channel(data_channel)

    def set_session_id(self, session_id: str) -> None:
        """ì„¸ì…˜ ID ì„¤ì • ë° ì„¸ì…˜ ì €ì¥ì†Œì˜ í•„í„°ë¥¼ ì¦‰ì‹œ ë°˜ì˜"""
        self.session_id = session_id
        # ì„¸ì…˜ ë³€ê²½ ì‹œ, í´ë˜ìŠ¤ë³„ seen ID ì´ˆê¸°í™”
        self.seen_track_ids_by_class = {}
        try:
            category_flags = session_state_manager.get_video_filter(session_id)
            enabled_ids = _categories_to_enabled_class_ids(category_flags)
            self.detection_filter.use_class_filter = True
            self.detection_filter.set_class_filter(enabled_ids)
            print(f"ğŸ¯ ì„¸ì…˜ {session_id} í´ë˜ìŠ¤ í•„í„° ì ìš©: {enabled_ids}")
            # ë¸”ëŸ¬ í”Œë˜ê·¸ ì ìš©
            self._apply_blur_flag_from_session(session_id)
        except Exception as e:
            print(f"ì„¸ì…˜ í•„í„° ì ìš© ì‹¤íŒ¨(session {session_id}): {e}")

    def apply_video_filter_for_session(self, session_id: str) -> None:
        """ì™¸ë¶€ì—ì„œ í˜¸ì¶œ: ì„¸ì…˜ì˜ ë¹„ë””ì˜¤ ì¹´í…Œê³ ë¦¬ ì„¤ì •ì„ ì½ì–´ í´ë˜ìŠ¤ í•„í„° ê°±ì‹ """
        try:
            category_flags = session_state_manager.get_video_filter(session_id)
            enabled_ids = _categories_to_enabled_class_ids(category_flags)
            self.detection_filter.use_class_filter = True
            self.detection_filter.set_class_filter(enabled_ids)
            print(f"ğŸ¯ ì„¸ì…˜ {session_id} í´ë˜ìŠ¤ í•„í„° ê°±ì‹ : {enabled_ids}")
            # ë¸”ëŸ¬ í”Œë˜ê·¸ë„ í•¨ê»˜ ê°±ì‹ 
            self._apply_blur_flag_from_session(session_id)
        except Exception as e:
            print(f"ì„¸ì…˜ ë¹„ë””ì˜¤ í•„í„° ê°±ì‹  ì‹¤íŒ¨(session {session_id}): {e}")

    def _apply_blur_flag_from_session(self, session_id: str) -> None:
        """ì„¸ì…˜ ì €ì¥ì†Œì˜ videoFilter.action.filtering ê°’ì„ ì½ì–´ ë¸”ëŸ¬ on/off ì„¤ì •"""
        try:
            session_filter = session_state_manager.get_session_filter(session_id)
            video_filter = (session_filter or {}).get('videoFilter')
            action = (video_filter or {}).get('action') or {}
            blur_enable = bool(action.get('filtering', False))
            self.visualizer.enable_object_blur(enable=blur_enable, blur_classes=None, blur_strength=self.visualizer.blur_strength)
            if blur_enable:
                print(f"ğŸ”’ ì„¸ì…˜ {session_id} ë¸”ëŸ¬ í™œì„±í™” (filtering=true)")
            else:
                print(f"ğŸ”“ ì„¸ì…˜ {session_id} ë¸”ëŸ¬ ë¹„í™œì„±í™” (filtering=false)")
        except Exception as e:
            print(f"ì„¸ì…˜ ë¸”ëŸ¬ í”Œë˜ê·¸ ì ìš© ì‹¤íŒ¨(session {session_id}): {e}")
    
    def process_detection_results(self):
        """
        ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ê°ì§€ ê²°ê³¼ë¥¼ Data Channelë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
        í´ë˜ìŠ¤ë³„ë¡œ ìƒˆë¡œìš´ ByteTrack IDê°€ ë“±ì¥í•  ë•Œë§Œ 1íšŒ ì „ì†¡í•©ë‹ˆë‹¤.
        """
        if not self.data_channel or not self.current_detections:
            return
        
        try:
            # í˜„ì¬ ê°ì§€ ê²°ê³¼ë¥¼ ë³µì‚¬í•˜ì—¬ ì²˜ë¦¬
            detections = self.current_detections.copy()
            sent_any = False
            # í˜„ì¬ ì‹œê°„ì„ HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            current_time = datetime.now()
            time_str = current_time.strftime("%H:%M:%S")
            
            for detection in detections:
                # ByteTrack IDê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                if detection.track_id is None:
                    continue
                class_id = detection.class_id
                track_id = detection.track_id
                # í´ë˜ìŠ¤ë³„ë¡œ ì´ë¯¸ ë³¸ IDì¸ì§€ í™•ì¸
                seen_set = self.seen_track_ids_by_class.get(class_id)
                if seen_set is None:
                    seen_set = set()
                    self.seen_track_ids_by_class[class_id] = seen_set
                if track_id in seen_set:
                    continue
                # ìƒˆë¡œìš´ ID â†’ ì „ì†¡í•˜ê³  ê¸°ë¡
                seen_set.add(track_id)
                sent_any = True
                # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸°
                category = CLASS_CATEGORY_MAPPING.get(detection.class_id, 'ê¸°íƒ€')
                detail = CLASS_NAMES.get(detection.class_id, 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                # ê° ê°ì§€ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì „ì†¡
                message = {
                    'type': 'video',
                    'category': category,
                    'detail': detail,
                    'time': time_str,
                }
                
                # JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ì—¬ ì „ì†¡
                try:
                    json_message = json.dumps(message, ensure_ascii=False)
                    self.data_channel.send(json_message)
                    print(f"ğŸ“¨ ì‹ ê·œ ID ê°ì§€ ì „ì†¡: class={class_id} id={track_id} category={category} detail={detail}")
                except Exception as json_error:
                    print(f"âŒ JSON ì§ë ¬í™” ì˜¤ë¥˜: {json_error}")
                    # ê°„ë‹¨í•œ ë©”ì‹œì§€ë¡œ ì¬ì‹œë„
                    simple_message = {
                        'type': 'video',
                        'category': category,
                        'detail': detail,
                        'time': time_str
                    }
                    try:
                        json_message = json.dumps(simple_message, ensure_ascii=False)
                        self.data_channel.send(json_message)
                        print(f"ğŸ“¨ ì‹ ê·œ ID ê°„ë‹¨ ì „ì†¡: class={class_id} id={track_id} category={category}")
                    except Exception as retry_error:
                        print(f"âŒ ì¬ì‹œë„ ì „ì†¡ ì˜¤ë¥˜: {retry_error}")
            
            # ì „ì†¡ ì™„ë£Œ í›„ ê°ì§€ ê²°ê³¼ ì´ˆê¸°í™” (ì¤‘ë³µ ì „ì†¡ ë°©ì§€)
            if sent_any:
                self.current_detections = []
            
        except Exception as e:
            print(f"âŒ Data Channel ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def add_detection_callback(self, callback: Callable[[List[DetectionResult]], None]):
        """
        ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        @param {Callable} callback - ê°ì§€ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•  ì½œë°± í•¨ìˆ˜
        """
        self.detection_callbacks.append(callback)
    
    def remove_detection_callback(self, callback: Callable[[List[DetectionResult]], None]):
        """
        ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        @param {Callable} callback - ì œê±°í•  ì½œë°± í•¨ìˆ˜
        """
        if callback in self.detection_callbacks:
            self.detection_callbacks.remove(callback)
    
    def set_detection_filter(self, enabled_classes: Optional[List[int]] = None, 
                           confidence_range: Optional[tuple] = None,
                           area_range: Optional[tuple] = None):
        """
        ê°ì§€ í•„í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {List[int]} enabled_classes - í™œì„±í™”í•  í´ë˜ìŠ¤ ID ëª©ë¡
        @param {tuple} confidence_range - ì‹ ë¢°ë„ ë²”ìœ„ (min, max)
        @param {tuple} area_range - ë©´ì  ë²”ìœ„ (min, max)
        """
        if enabled_classes is not None:
            self.detection_filter.set_class_filter(enabled_classes)
        
        if confidence_range is not None:
            min_conf, max_conf = confidence_range
            self.detection_filter.set_confidence_range(min_conf, max_conf)
        
        if area_range is not None:
            min_area, max_area = area_range
            self.detection_filter.set_area_range(min_area, max_area)
    
    def get_current_detections(self) -> List[DetectionResult]:
        """
        í˜„ì¬ í”„ë ˆì„ì˜ ê°ì§€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {List[DetectionResult]} í˜„ì¬ ê°ì§€ ê²°ê³¼
        """
        return self.current_detections.copy()
    
    def get_detection_stats(self) -> Dict:
        """
        ë¬¼ì²´ ê°ì§€ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {Dict} ê°ì§€ í†µê³„
        """
        if not self.object_detector:
            return {}
        
        stats = self.object_detector.get_stats()
        stats.update({
            'detection_time': self.processing_stats['detection_time'],
            'objects_detected': self.processing_stats['objects_detected']
        })
        return stats
    
    def reset_detection_stats(self):
        """
        ë¬¼ì²´ ê°ì§€ í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        if self.object_detector:
            self.object_detector.reset_stats()
        
        self.processing_stats['detection_time'] = 0.0
        self.processing_stats['objects_detected'] = 0
    
    def enable_object_blur(self, enable: bool = True, blur_classes: Optional[List[int]] = None, blur_strength: int = 15):
        """
        ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {bool} enable - ë¸”ëŸ¬ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        @param {List[int]} blur_classes - ë¸”ëŸ¬ ì ìš©í•  í´ë˜ìŠ¤ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  í´ë˜ìŠ¤)
        @param {int} blur_strength - ë¸”ëŸ¬ ê°•ë„ (í™€ìˆ˜ ê¶Œì¥)
        """
        self.visualizer.enable_object_blur(enable, blur_classes, blur_strength)
        if enable:
            print(f"ğŸ”’ ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ê°•ë„: {blur_strength})")
            if blur_classes:
                print(f"   ë¸”ëŸ¬ ì ìš© í´ë˜ìŠ¤: {blur_classes}")
            else:
                print("   ëª¨ë“  ê°ì§€ëœ ë¬¼ì²´ì— ë¸”ëŸ¬ ì ìš©")
        else:
            print("ğŸ”“ ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


class VideoEchoTrack(MediaStreamTrack):
    """
    ë¹„ë””ì˜¤ ì—ì½” íŠ¸ë™ í´ë˜ìŠ¤
    
    í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë°›ì€ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê³  ë‹¤ì‹œ ì „ì†¡í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    kind = "video"
    data_channel = None
    
    def __init__(self, track):
        """
        VideoEchoTrack ì´ˆê¸°í™”
        
        @param {MediaStreamTrack} track - ì›ë³¸ ë¹„ë””ì˜¤ íŠ¸ë™
        """
        super().__init__()
        self.track = track
        self.video_processor = VideoProcessor()
        
        # í”„ë ˆì„ ì „ì†¡ì„ ìœ„í•œ ìƒíƒœ ê´€ë¦¬
        self._last_sent_frame = None  # ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì†¡í•œ í”„ë ˆì„
        self._frame_interval = 1.0 / 30.0  # 30fps ê¸°ì¤€ (ì•½ 33ms)
        self._last_send_time = 0.0
        self._processing_task: Optional[asyncio.Task] = None
        self._closed: bool = False
        # PTS ê¸°ë°˜ ì¬ìƒ íƒ€ì„ë¼ì¸ ìƒíƒœ
        self._playout_start_pts: Optional[int] = None
        self._playout_start_time: float = 0.0
        # ë³´ë¥˜ ì¤‘ì¸ í”„ë ˆì„(ì‹œê°„ ë„ë˜ ì „ ë¯¸ë¦¬ êº¼ë‚¸ í”„ë ˆì„ ë³´ê´€)
        self._pending_frame: Optional[VideoFrame] = None
        self._pending_target_time: float = 0.0
        
        # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ë£¨í”„ ì‹œì‘
        loop = asyncio.get_event_loop()
        self._processing_task = loop.create_task(self._processing_loop())
    
    async def recv(self):
        """
        ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì´ì „ í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ ì „ì†¡í•©ë‹ˆë‹¤.
        """
        current_time = time()
        
        # í”„ë ˆì„ ì „ì†¡ ê°„ê²© ì²´í¬ (30fps ì œí•œ)
        if current_time - self._last_send_time < self._frame_interval:
            # ì´ì „ í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ ì „ì†¡
            if self._last_sent_frame is not None:
                return self._last_sent_frame
        
        # ë³´ë¥˜ í”„ë ˆì„ì´ ìˆê³  ì•„ì§ ì¬ìƒ ì‹œê°ì´ ì•„ë‹ˆë©´ ì´ì „ í”„ë ˆì„ ìœ ì§€
        if self._pending_frame is not None:
            if current_time + 0.0005 < self._pending_target_time and self._last_sent_frame is not None:
                return self._last_sent_frame
            else:
                # ì‹œê° ë„ë˜ â†’ ë³´ë¥˜ í”„ë ˆì„ ì†¡ì¶œ
                processed_frame = self._pending_frame
                self._pending_frame = None
                self._pending_target_time = 0.0
                self._last_sent_frame = processed_frame
                self._last_send_time = current_time
                self.video_processor.process_detection_results()
                return processed_frame

        # VideoProcessorì—ì„œ ì²˜ë¦¬ëœ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
        processed_frame = self.video_processor.get_processed_frame()
        
        if processed_frame is not None:
            # PTS ê¸°ë°˜ ì¬ìƒ íƒ€ì„ë¼ì¸ ë§¤í•‘
            try:
                frame_pts = processed_frame.pts
                frame_tb = processed_frame.time_base
                if frame_pts is not None and frame_tb is not None:
                    if self._playout_start_pts is None:
                        self._playout_start_pts = frame_pts
                        self._playout_start_time = current_time
                    delta_pts = frame_pts - self._playout_start_pts
                    target_time = self._playout_start_time + (delta_pts * float(frame_tb))
                    if current_time + 0.0005 < target_time and self._last_sent_frame is not None:
                        # ì•„ì§ ì¬ìƒ ì‹œê°ì´ ì•„ë‹ˆë©´ ë³´ë¥˜ í›„ ì§ì „ í”„ë ˆì„ ìœ ì§€
                        self._pending_frame = processed_frame
                        self._pending_target_time = target_time
                        return self._last_sent_frame
                # ì¦‰ì‹œ ì†¡ì¶œ
            except Exception:
                pass

            self._last_sent_frame = processed_frame
            self._last_send_time = current_time
            
            # ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ê°ì§€ ê²°ê³¼ ì²˜ë¦¬
            self.video_processor.process_detection_results()
            
            # print(f"ğŸ“¤ ì²˜ë¦¬ëœ í”„ë ˆì„ ì „ì†¡: {self.video_processor.frame_count}")
            return processed_frame
        else:
            # ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ì›ë³¸ í”„ë ˆì„ ì‚¬ìš©
            original_frame = await self.track.recv()
            
            # PTS ë¬¸ì œ í•´ê²°: í”„ë ˆì„ ë³µì œ ì‹œ ì˜¬ë°”ë¥¸ PTS ì„¤ì •
            if self._last_sent_frame is None:
                # ì²« ë²ˆì§¸ í”„ë ˆì„ì¸ ê²½ìš° ì›ë³¸ ì‚¬ìš©
                self._last_sent_frame = original_frame
                self._last_send_time = current_time
                print(f"ğŸ“¤ ì›ë³¸ í”„ë ˆì„ ì „ì†¡ (ì²˜ë¦¬ëœ í”„ë ˆì„ ì—†ìŒ)")
                return original_frame
            else:
                                 # ì´ì „ í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ ì „ì†¡í•˜ë˜, ì˜¬ë°”ë¥¸ PTS ì„¤ì •
                 # VideoFrameì„ numpy ë°°ì—´ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ VideoFrameìœ¼ë¡œ ìƒì„±
                 img_array = self._last_sent_frame.to_ndarray(format='bgr24')
                 cloned_frame = VideoFrame.from_ndarray(img_array, format='bgr24')
                 cloned_frame.pts = original_frame.pts
                 cloned_frame.time_base = original_frame.time_base
                 # rate ì†ì„±ì€ VideoFrameì— ì—†ìœ¼ë¯€ë¡œ ì œê±°
                 self._last_send_time = current_time
                 print(f"ğŸ“¤ ì´ì „ í”„ë ˆì„ ë³µì œ ì „ì†¡ (ì²˜ë¦¬ëœ í”„ë ˆì„ ì—†ìŒ)")
                 return cloned_frame

    async def _processing_loop(self) -> None:
        """ì›ë³¸ íŠ¸ë™ì—ì„œ í”„ë ˆì„ì„ ì§€ì†ì ìœ¼ë¡œ ì½ì–´ ë³„ë„ ìŠ¤ë ˆë“œì— ì „ë‹¬."""
        try:
            while not self._closed:
                frame = await self.track.recv()
                try:
                    # VideoProcessorì— í”„ë ˆì„ ì „ë‹¬ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬)
                    self.video_processor.process_frame(frame)
                except Exception as e:
                    print(f"âš ï¸ ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        except asyncio.CancelledError:
            # ì •ìƒì ì¸ ì·¨ì†Œ íë¦„
            return
        except Exception as loop_error:
            if not self._closed:
                print(f"âš ï¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {loop_error}")

    def stop(self) -> None:
        """íŠ¸ë™ ì¤‘ì§€ ì‹œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì¢…ë£Œ ë° ìì› ì •ë¦¬."""
        if self._closed:
            return
        self._closed = True
        
        # VideoProcessorì˜ ë³„ë„ ìŠ¤ë ˆë“œ ì¤‘ì§€
        if hasattr(self.video_processor, '_stop_processing_thread'):
            self.video_processor._stop_processing_thread()
        
        try:
            if self._processing_task:
                self._processing_task.cancel()
                self._processing_task = None
        except Exception:
            pass
        try:
            super().stop()
        except Exception:
            pass
    
    def get_processing_stats(self) -> dict:
        """
        ë¹„ë””ì˜¤ ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {dict} ì²˜ë¦¬ í†µê³„
        """
        return self.video_processor.get_stats()
    
    def reset_processing_stats(self):
        """
        ë¹„ë””ì˜¤ ì²˜ë¦¬ í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.video_processor.reset_stats()
    
    def add_detection_callback(self, callback: Callable[[List[DetectionResult]], None]):
        """
        ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        @param {Callable} callback - ê°ì§€ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•  ì½œë°± í•¨ìˆ˜
        """
        self.video_processor.add_detection_callback(callback)
    
    def remove_detection_callback(self, callback: Callable[[List[DetectionResult]], None]):
        """
        ë¬¼ì²´ ê°ì§€ ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        @param {Callable} callback - ì œê±°í•  ì½œë°± í•¨ìˆ˜
        """
        self.video_processor.remove_detection_callback(callback)
    
    def set_detection_filter(self, enabled_classes: Optional[List[int]] = None, 
                           confidence_range: Optional[tuple] = None,
                           area_range: Optional[tuple] = None):
        """
        ê°ì§€ í•„í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {List[int]} enabled_classes - í™œì„±í™”í•  í´ë˜ìŠ¤ ID ëª©ë¡
        @param {tuple} confidence_range - ì‹ ë¢°ë„ ë²”ìœ„ (min, max)
        @param {tuple} area_range - ë©´ì  ë²”ìœ„ (min, max)
        """
        self.video_processor.set_detection_filter(enabled_classes, confidence_range, area_range)
    
    def get_current_detections(self) -> List[DetectionResult]:
        """
        í˜„ì¬ í”„ë ˆì„ì˜ ê°ì§€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {List[DetectionResult]} í˜„ì¬ ê°ì§€ ê²°ê³¼
        """
        return self.video_processor.get_current_detections()
    
    def get_detection_stats(self) -> Dict:
        """
        ë¬¼ì²´ ê°ì§€ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns {Dict} ê°ì§€ í†µê³„
        """
        return self.video_processor.get_detection_stats()
    
    def reset_detection_stats(self):
        """
        ë¬¼ì²´ ê°ì§€ í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.video_processor.reset_detection_stats()
    
    def enable_object_blur(self, enable: bool = True, blur_classes: Optional[List[int]] = None, blur_strength: int = 15):
        """
        ë¬¼ì²´ ë¸”ëŸ¬ ê¸°ëŠ¥ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {bool} enable - ë¸”ëŸ¬ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        @param {List[int]} blur_classes - ë¸”ëŸ¬ ì ìš©í•  í´ë˜ìŠ¤ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  í´ë˜ìŠ¤)
        @param {int} blur_strength - ë¸”ëŸ¬ ê°•ë„ (í™€ìˆ˜ ê¶Œì¥)
        """
        self.video_processor.enable_object_blur(enable, blur_classes, blur_strength)
    
    def set_data_channel(self, data_channel):
        """
        Data Channelì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        @param {RTCDataChannel} data_channel - ì„¤ì •í•  Data Channel
        """
        self.data_channel = data_channel
        print(f"ğŸ“¡ VideoEchoTrackì— Data Channel ì„¤ì •ë¨: {data_channel.label}")
        
        # VideoProcessorì—ë„ Data Channel ì „ë‹¬
        if hasattr(self.video_processor, 'set_data_channel'):
            self.video_processor.set_data_channel(data_channel)
