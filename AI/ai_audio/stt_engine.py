"""
ìŒì„± ì¸ì‹ ì—”ì§„ ëª¨ë“ˆ

@module stt_engine
@author HeeGyeong
@date 2025-08-16
@description Whisper Small + Faster-Whisperë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì—”ì§„
"""

import asyncio
import threading
import queue
import time
import os
import numpy as np
from typing import Callable, Optional
from collections import deque
import soundfile as sf
from datetime import datetime

# HuggingFace Hub ìµœì í™” ì„¤ì •
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'  # symlink ê²½ê³  ë¹„í™œì„±í™”

try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    print("âš ï¸ faster-whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install faster-whisper")
    FASTER_WHISPER_AVAILABLE = False

def check_cuda_compatibility():
    """
    CUDA í˜¸í™˜ì„±ì„ ìžì„¸ížˆ í™•ì¸í•©ë‹ˆë‹¤.
    """
    print("ï¿½ï¿½ CUDA í˜¸í™˜ì„± ìƒì„¸ í™•ì¸:")
    print("-" * 40)
    
    # 1. PyTorch CUDA ìƒíƒœ
    try:
        import torch
        print(f"ï¿½ï¿½ PyTorch ë²„ì „: {torch.__version__}")
        
        if torch.cuda.is_available():
            print("âœ… PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥")
            
            # GPU ìƒì„¸ ì •ë³´
            device = torch.cuda.current_device()
            gpu_props = torch.cuda.get_device_properties(device)
            
            print(f"ðŸŽ® GPU: {gpu_props.name}")
            print(f"ï¿½ï¿½ CUDA Capability: {gpu_props.major}.{gpu_props.minor}")
            print(f"ðŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_props.total_memory / 1024**3:.1f}GB")
            print(f"ï¿½ï¿½ ë©€í‹°í”„ë¡œì„¸ì„œ: {gpu_props.multi_processor_count}")
            
            # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
            try:
                test_tensor = torch.randn(1000, 1000).cuda()
                memory_used = torch.cuda.memory_allocated()
                print(f"ðŸ§ª GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {memory_used / 1024**2:.1f}MB ì‚¬ìš©")
                del test_tensor
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"âŒ GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                
        else:
            print("âŒ PyTorch CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥")
            
    except ImportError:
        print("âŒ PyTorch ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # 2. Faster-Whisper CUDA ì§€ì›
    print("\nðŸŽ¤ Faster-Whisper CUDA ì§€ì›:")
    try:
        # GPU ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        print("ðŸ” GPU ëª¨ë“œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
        
        test_model = WhisperModel(
            "tiny",
            device="cuda",
            compute_type="int8",
            download_root="./models"
        )
        
        print("âœ… GPU ëª¨ë“œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        try:
            import numpy as np
            test_audio = np.random.randn(16000).astype(np.float32)  # 1ì´ˆ í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤
            
            print("ðŸ§ª GPU ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
            segments, _ = test_model.transcribe(test_audio, language="ko")
            print("âœ… GPU ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            
        except Exception as e:
            print(f"âŒ GPU ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        del test_model
        
    except Exception as e:
        print(f"âŒ GPU ëª¨ë“œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   â†’ CPU fallback í•„ìš”")
    
    print("-" * 40)


class StreamingSpeechRecognizer:
    """
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìŒì„± ì¸ì‹ í´ëž˜ìŠ¤
    
    WebRTC ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ì„œ 3ì´ˆ ë²„í¼ë§ í›„ Whisperë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 model_size: str = "small",
                 language: str = "ko",
                 buffer_duration: float = 3.0,
                 sample_rate: int = 16000,
                 on_result: Optional[Callable[[str], None]] = None):
        """
        StreamingSpeechRecognizer ì´ˆê¸°í™”
        
        @param model_size: Whisper ëª¨ë¸ í¬ê¸° ("tiny", "small", "medium")
        @param language: ì¸ì‹í•  ì–¸ì–´ ì½”ë“œ
        @param buffer_duration: ë²„í¼ë§ ì‹œê°„ (ì´ˆ)
        @param sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        @param on_result: ì¸ì‹ ê²°ê³¼ ì½œë°± í•¨ìˆ˜
        """
        # cuda í˜¸í™˜ì„± í™•ì¸
        # check_cuda_compatibility()


        if not FASTER_WHISPER_AVAILABLE:
            raise ImportError("faster-whisper íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.model_size = model_size
        self.language = language
        self.buffer_duration = buffer_duration
        self.sample_rate = sample_rate
        self.on_result = on_result
        
        # ì²˜ë¦¬ ìƒíƒœ
        self.is_running = False
        self.processing_thread = None
        self.audio_queue = queue.Queue(maxsize=30)  # í ì‚¬ì´ì¦ˆ ì¦ê°€ (10 â†’ 30)
        
        # í†µê³„
        self.stats = {
            'total_processed': 0,
            'total_processing_time': 0.0,
            'last_result': '',
            'buffer_overflow_count': 0
        }

        # Whisper ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
        

    
    def _initialize_model(self):
        """
        Whisper ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        try:

            # CPU ì‚¬ìš©
            self.model = WhisperModel(
                self.model_size,
                device="cpu",
                # device="cuda",
                compute_type="int8",  # CPU ìµœì í™”
                cpu_threads=4,
                download_root="./models"
            )                
            print(f"âœ… Whisper {self.model_size} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
            
            # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤ í™•ì¸
            try:
                # faster-whisper ëª¨ë¸ì˜ ì‹¤ì œ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
                if hasattr(self.model, 'model'):
                    print(f"ðŸ” ë‚´ë¶€ ëª¨ë¸ ë¡œë“œë¨")
                    
                # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë° GPU ì •ë³´ í™•ì¸
                try:
                    import torch
                    if torch.cuda.is_available():
                        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
                        print(f"ðŸ” CUDA ë²„ì „: {torch.version.cuda}")
                        print(f"ðŸ” GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
                        print(f"ðŸŽ¯ GPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
                    else:
                        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPUë¡œ fallbackë¨")
                        print(f"ðŸŽ¯ CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
                except ImportError:
                    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - CUDA ìƒíƒœ í™•ì¸ ë¶ˆê°€")
                    print(f"ðŸŽ¯ ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: cuda (ì‹¤ì œ ìƒíƒœëŠ” faster-whisperì— ì˜í•´ ê²°ì •ë¨)")
                    
            except Exception as debug_e:
                print(f"âš ï¸ ë””ë²„ê·¸ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {debug_e}")
            
        except Exception as e:
            print(f"âŒ Whisper ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def start_recognition(self):
        """
        ìŒì„± ì¸ì‹ì„ ì‹œìž‘í•©ë‹ˆë‹¤.
        """
        if self.is_running:
            print("âš ï¸ ì´ë¯¸ ìŒì„± ì¸ì‹ì´ ì‹¤í–‰ ì¤‘ìž…ë‹ˆë‹¤")
            return
        
        self.is_running = True
        self.processing_thread = threading.Thread(
            target=self._processing_worker,
            daemon=True
        )
        self.processing_thread.start()
        
        print("ðŸŽ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ì´ ì‹œìž‘ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def stop_recognition(self):
        """
        ìŒì„± ì¸ì‹ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.
        """
        if not self.is_running:
            return
        
        self.is_running = False
        
        if self.processing_thread:
            self.processing_thread.join(timeout=2.0)
        
        print("ðŸ”‡ ìŒì„± ì¸ì‹ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def process_audio_chunk(self, audio_data: dict):
        """
        ë¯¸ë¦¬ ì±„ì›Œì§„ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ë°”ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        @param frame_info: ì˜¤ë””ì˜¤ ë°ì´í„°ì™€ íƒ€ìž„ìŠ¤íƒ·í”„ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_running:
            print("âš ï¸ STT ì—”ì§„ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹˜")
            return
        
        print(f"ðŸ“¥ STT ì—”ì§„ ìˆ˜ì‹ : {len(audio_data)} ìƒ˜í”Œ (3ì´ˆ ë²„í¼ ì™„ì„±ë¨)")
        
        # 3ì´ˆ ë²„í¼ë¥¼ ë°”ë¡œ ì²˜ë¦¬ íì— ì¶”ê°€ (ë‚´ë¶€ ë²„í¼ë§ ìƒëžµ)
        try:
            self.audio_queue.put_nowait(audio_data)
            print(f"âœ… 3ì´ˆ ì˜¤ë””ì˜¤ íì— ë°”ë¡œ ì¶”ê°€ë¨. í í¬ê¸°: {self.audio_queue.qsize()}")
        except queue.Full:
            # íê°€ ê°€ë“ ì°¬ ê²½ìš°, ê°€ìž¥ ì˜¤ëž˜ëœ ì•„ì´í…œì„ ì œê±°í•˜ê³  ìƒˆ ì•„ì´í…œ ì¶”ê°€
            try:
                old_data = self.audio_queue.get_nowait()
                self.audio_queue.put_nowait(audio_data)
                print(f"âš ï¸ í ê°€ë“ì°¸ - ì˜¤ëž˜ëœ ì²­í¬ ì œê±° í›„ ìƒˆ ì²­í¬ ì¶”ê°€. í í¬ê¸°: {self.audio_queue.qsize()}")
                self.stats['buffer_overflow_count'] += 1
            except (queue.Empty, queue.Full):
                print("âš ï¸ ì²˜ë¦¬ íê°€ ê°€ë“í•¨ - 3ì´ˆ ì˜¤ë””ì˜¤ ì²­í¬ ë“œë¡­")
                self.stats['buffer_overflow_count'] += 1

    
    def _processing_worker(self):
        """
        ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì›Œì»¤ ìŠ¤ë ˆë“œ
        """
        print("ðŸ”„ ìŒì„± ì¸ì‹ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œìž‘")
        
        while self.is_running:
            try:
                # print(f"ðŸ” íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ëŒ€ê¸° ì¤‘... (í í¬ê¸°: {self.audio_queue.qsize()})")
                # íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (íƒ€ìž„ì•„ì›ƒ 1ì´ˆ)
                audio_data = self.audio_queue.get(timeout=1.0)
                # print(f"ðŸŽµ íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ë°›ìŒ: {len(audio_data['audio_data'])} ìƒ˜í”Œ")
                
                # Whisperë¡œ ìŒì„± ì¸ì‹ ì²˜ë¦¬
                self._process_with_whisper(audio_data)
                self.audio_queue.task_done()
                
            except queue.Empty:
                # print("â° í íƒ€ìž„ì•„ì›ƒ (1ì´ˆ) - ê³„ì† ëŒ€ê¸°")
                continue
            except Exception as e:
                print(f"âŒ ìŒì„± ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
        
        print("ðŸ›‘ ìŒì„± ì¸ì‹ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì¢…ë£Œ")
    
    def _process_with_whisper(self, audio_data: dict):
        """
        Whisperë¥¼ ì‚¬ìš©í•´ ìŒì„±ì„ ì¸ì‹í•©ë‹ˆë‹¤.
        
        @param audio_data: ì²˜ë¦¬í•  ì˜¤ë””ì˜¤ ë°ì´í„° (3ì´ˆ)
        """
        start_time = time.time()
        audio_np = audio_data['audio_data']
        timestamp = audio_data.get('timestamp')
        
        try:
            print(f"ðŸŽ¤ Whisper ì²˜ë¦¬ ì‹œìž‘: {len(audio_np)} ìƒ˜í”Œ, ë°ì´í„° íƒ€ìž…: {audio_np.dtype}")
            print(f"ðŸ”Š ì˜¤ë””ì˜¤ ë ˆë²¨: min={audio_np.min():.3f}, max={audio_np.max():.3f}, rms={np.sqrt(np.mean(audio_np**2)):.3f}")

            # Whisperë¡œ ìŒì„± ì¸ì‹ (ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ë¹„í™œì„±í™”)
            segments, _ = self.model.transcribe(
                audio_np,
                language=self.language,
                beam_size=1,  # ì†ë„ ìš°ì„ 
                best_of=1,
                vad_filter=False,  # VAD í•„í„° ë¹„í™œì„±í™” â†’ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ìµœì†Œí™”
                word_timestamps=False,  # ë‹¨ì–´ë³„ íƒ€ìž„ìŠ¤íƒ¬í”„ ë¹„í™œì„±í™”
                # vad_parameters=dict(
                #     min_silence_duration_ms=500,
                #     speech_pad_ms=400
                # )
            )
            
            # ì¸ì‹ ê²°ê³¼ ì¶”ì¶œ (ì´í„°ë ˆì´í„°ë¥¼ í•œ ë²ˆë§Œ ì‚¬ìš©)
            segments_list = list(segments)  # ì´í„°ë ˆì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            # print(f"ðŸ” Whisper ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(segments_list)}")
            
            text_result = " ".join(segment.text.strip() for segment in segments_list)
            
            processing_time = time.time() - start_time
            
            print(f"â±ï¸ Whisper ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"ðŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: '{text_result}'")
            
            if text_result and text_result.strip():
                # print(f"ðŸŽ¯ ì¸ì‹ ê²°ê³¼ ({processing_time:.2f}s): {text_result}")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats['total_processed'] += 1
                self.stats['total_processing_time'] += processing_time
                self.stats['last_result'] = text_result
                
                # ì½œë°± í˜¸ì¶œ
                if self.on_result:
                    self.on_result(text_result, timestamp)

            else:
                print(f"ðŸ”‡ ìŒì„± ì—†ìŒ ë˜ëŠ” ë¹ˆ ê²°ê³¼ ({processing_time:.2f}s)")
            
        except Exception as e:
            print(f"âŒ Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def _process_current_buffer(self):
        """
        í˜„ìž¬ ë²„í¼ì˜ ë‚´ìš©ì„ ê°•ì œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ì¢…ë£Œ ì‹œ í˜¸ì¶œ)
        """
        if self.current_buffer_size > 0:
            buffer_audio = np.array(list(self.audio_buffer))
            try:
                self.audio_queue.put_nowait(buffer_audio)
            except queue.Full:
                pass

    
    def get_final_result(self) -> str:
        """
        ìµœì¢… ì¸ì‹ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns: ë§ˆì§€ë§‰ ì¸ì‹ ê²°ê³¼
        """
        return self.stats.get('last_result', '')
    
    def get_stats(self) -> dict:
        """
        ìŒì„± ì¸ì‹ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        @returns: í†µê³„ ì •ë³´
        """
        stats = self.stats.copy()
        if stats['total_processed'] > 0:
            stats['avg_processing_time'] = (
                stats['total_processing_time'] / stats['total_processed']
            )
        return stats
    
    def reset_stats(self):
        """
        í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.stats = {
            'total_processed': 0,
            'total_processing_time': 0.0,
            'last_result': '',
            'buffer_overflow_count': 0
        }
